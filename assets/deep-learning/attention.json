{"title": "Attention Mechanism Explained", "summary": "In this article we will explain different types of attention mechanism", "author": "Junxiao Guo", "date": "2023-04-09", "tags": ["deep-learning", "transformer", "attention", "llm"], "content": "---\ntitle: Attention Mechanism Explained\nsummary: In this article we will explain different types of attention mechanism \nauthor: Junxiao Guo\ndate: 2023-04-09\ntags:\n  - deep-learning\n  - transformer\n  - attention\n  - llm\n---\n\nIn this article we will explain different types of attention mechanism:\n\n- Dot-product (encoder-decoder) attention\n- Self attention\n- Multi-head attention\n- Bi-directional attention\n\n## Dot-Product Attention\n\nThe first attention mechanism we will focus on is dot-product (encoder-decoder) attention. When we perform many NLP tasks we would typically convert a word into a vector (word2vec), with transformers we perform the same operation. These vectors allows us to represent meaning numerically (eg days of the week may be clustered together, or we can perform logical arithmetic on the vectors - $King - Man + Woman = Queen$).\n\nBecause of this, we would expect sentences with similar meaning to have a similar set of values. For example, in neural machine translation, the phrase *\"Hello, how are you?\"*, and the Italian equivalent *\"Ciao, come va?\"* should share a similar matrix representation.\n\nNow, when we iterate through each word, and compare the individual vectors between the two sequences - we should find that words such as \"Hello\" and \"Ciao\" have higher similarity than words that have different meaning such as \"are\" and \"Ciao\".\n\n![nmt_attention](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/nmt_attention.png?raw=true)\n\nWe can display this mapping between word attentions better using a heatmap:\n\n\n![attention_heatmatp](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/attention_heatmap.png?raw=true)\n\n## Alignment\n\nThis similarity between word vectors is known as 'alignment', and we calculate alignment between two vectors using the dot product. The dot product formula looks like:\n\n$$u\\cdot{v} = \\lvert A \\rvert \\lvert B \\rvert \\cos{\\theta} = \\sum^{n}_{i=1}{a_nb_n}$$\n\nSo, let's imagine rather than a high-dimensional vector (as we would usually expect with word vectors), we have simple, three-dimensional vectors which can be visualized. We have three of these vectors each representing a word:\n\n\n```python\nhello = [0.71, 0.14, 0.51]\nhi = [0.69, 0.15, 0.48]\ntomato = [0.16, 0.59, 0.49]\n```\n\nWe can plot each of these onto a 3D chart:\n\n\n```python\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(16,12))\nax = fig.add_subplot(projection='3d')\n\nxs = [hello[0], hi[0], tomato[0]]\nys = [hello[1], hi[1], tomato[1]]\nzs = [hello[2], hi[2], tomato[2]]\n\nax.scatter(xs, ys, zs, s=100)\n\nax.set_xlim((0, 1))\nax.set_ylim((0, 1))\nax.set_zlim((0, 1))\n\nplt.show()\n```\n\n\n    \n![vector_3d_example](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/vector_3d_example.png?raw=true)\n    \n\n\nThe vectors for hello and hi share the same area, with tomato out to the upper-left. To measure alignment between each of these vectors, we calculate the dot product of each - we'll rename the vectors as otherwise the next part will look very messy:\n\na = hello = [0.71, 0.14, 0.51] b = hi = [0.69, 0.15, 0.48] c = tomato [0.16, 0.59, 0.49]\n\nNow, to calculate the dot product between each of these vectors we do:\n\n$$a\\cdot{b} = \\sum_{i=1}^{n}{a_nb_n} = (0.71*0.69) + (0.14*0.15) + (0.51*0.43)$$\n\n\n```python\nimport numpy as np\n\na = np.array(hello)\nb = np.array(hi)\nc = np.array(tomato)\n\nnp.matmul(a, b.T)\n```\n\n\n\n\n    0.7556999999999999\n\n\n\n\n```python\nnp.matmul(a, c.T)\n```\n\n\n\n\n    0.4461\n\n\n\n\n```python\nnp.matmul(b, c.T)\n```\n\n\n\n\n    0.4341\n\n\n\nClearly, vectors a and b have better alignment between themselves than either to with c, as would be expected. It is this exact calculation that we perform for calculating the alignment vector, which we then use to calculate the dot-product attention between two sequences - however, rather than comparing word-to-word, we are comparing every word across the whole sequence in a single parallel operation.\n\n\n\n## Queries, Keys, and Values\nIn dot-product attention, there are three tensors that we will be comparing. The `query Q, key K, and value V`. **K** and **V** are usually the same and are derived from (in our case) the English text, **Q** is another tensor which is derived from the Italian text.\n\nThe first thing we do is calculate the alignment vector between our English and Italian sequences, **K** and **Q** using the dot product, just as we did before.\n\n![dot_product_attention](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/dot_product_attention.png?raw=true)\n\nOnce we calculate the dot product, we apply a softmax function to convert the dot product alignment into probabilities. These are then multiplied by **V** to give us the attention tensor **z**.\n\n## Self Attention\nWith dot-product attention, we calculated the alignment between word vectors from two different sequences - perfect for translation. Self-attention takes a different approach, here we compare words to previous words in the same sequence. So, where with dot-product attention we took our queries **Q** and keys **K** from two different sequences, self-attention takes them from the same sequence. Transformer models that look at previous tokens and try to predict the next include both text generation, and summarization.\n\nSo, just like before with dot-product attention, we calculate the dot-product again - this time taking **Q** and **K** from the same sequence.\n\n![self_attention](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/self_attention.png?raw=true)\n\nAfter calculating the dot-product across all items in the sequence, we apply a mask to remove all values calculated for future words - leaving us with the dot-product between past words only. Next, we take the softmax just as before, and multiply the result by **V** to get our attention **Z**.\n\n## Multihead Attention\nMultihead attention allows us to build several representations of attention between words - so rather than calculating attention once, we calculate it several times, concatenate the results, and pass them through a linear layer.\n\n\nAnd if we were to look at the multi-head attention segment in more detail we would see this:\n\n![multihead-attention](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/multihead_attention.png?raw=true)\n\n## Bi-directional Attention\n\nWe've explored both dot-product attention, and self-attention. Where dot-product compared two sequences, and self attention compared previous tokens from the same sequence, bidirectional attention compares tokens from the same sequence in both directions, subsequent and previous. This is as simple as performing the exact same operation that we performed for self-attention, but excluding the masking operation - allowing each word to be mapped to every other word in the same sequence. So, we could call this bi-directional self* attention. This is particularly useful for masked language modeling - and is used in BERT (Bidirectional Encoder* Representations from Transformers) - bidirectional self-attention refers to the bidirectional encoder, or the BE of BERT.\n\n![bi-directional_attention](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/transformer/attention/bidirectional_attention.png?raw=true)\n"}