{"title": "MNIST Handwritten Digit Recognition", "summary": "Hands on small project to build a neural net to recognize hand written digits from MNIST dataset with PyTorch.", "author": "Junxiao Guo", "date": "2021-05-05", "tags": ["deep-learning", "computer-vision"], "content": "---\ntitle: MNIST Handwritten Digit Recognition\nsummary: Hands on small project to build a neural net to recognize hand written digits from MNIST dataset with PyTorch. \nauthor: Junxiao Guo\ndate: 2021-05-05\ntags:\n  - deep-learning\n  - computer-vision\n---\n\n## Introduction on MNIST\n\nThe MNIST dataset (Mixed National Institute of Standards and Technology database) is a large handwritten digital database collected by the National Institute of Standards and Technology, including a training set of 60,000 examples and a test set of 10,000 examples.\n![](https://bbs-img.huaweicloud.com/blogs/img/MNIST.png?raw=true)\n\nThis tutorial will use PyTorch to implement a simple neural network model to recognize handwritten digits\n\n## Initialize\n\n```python\nimport torch\nimport torchvision\n```\n\n## Setting model parameters\n\n```python\nn_epochs = 3 \nbatch_size_train = 64    \nbatch_size_test = 1000    \nlearning_rate = 0.01\nmomentum = 0.5\nlog_interval = 100 # Frequency for log printing\n```\n\n## Loading dataset\n\n```python\ntrain_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/dataset/', train=True, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_train, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/dataset/', train=False, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_test, shuffle=True)\n```\n\n```python\nexamples = enumerate(test_loader)\nbatch_idx, (example_data, example_targets) = next(examples)\n```\n\n```python\nexample_data.shape\n```\n\n    torch.Size([1000, 1, 28, 28])\n\n```python\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n  plt.xticks([])\n  plt.yticks([])\nfig\n```\n\n![](https://bbs-img.huaweicloud.com/blogs/img/6f1ff120ae0cac7ecce64bd72348e18b_405x267.png@900-0-90-f.png?raw=true)\n\n## Building Neural Net\n\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n```\n\n### Defining network structure\n\n```python\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x,dim=-1)\n```\n\n### CNN(Convolution Neural Network) Version\n\n```python\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n#         self.conv2_drop = nn.Dropout2d()\n#         self.fc1 = nn.Linear(320, 50)\n#         self.fc2 = nn.Linear(50, 10)\n\n#     def forward(self, x):\n#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n#         x = x.view(-1, 320)\n#         x = F.relu(self.fc1(x))\n#         x = F.dropout(x, training=self.training)\n#         x = self.fc2(x)\n#         return F.log_softmax(x,dim=-1)\n```\n\n### Defining the output path\n\n```python\nimport os\ncur_dir = os.getcwd()\noutput_path = os.path.join(cur_dir,\"results\")\nfrom pathlib import Path\nPath(output_path).mkdir(parents=True, exist_ok=True)\n```\n\n### Instantiate the Net object and choose the optimizer\n\n```python\nnetwork = Net()\noptimizer = optim.SGD(network.parameters(), lr=learning_rate,\n                      momentum=momentum)\n```\n\n```python\ntrain_losses = []\ntrain_counter = []\ntest_losses = []\ntest_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n\n```\n\n### Training & Testing\n\n```python\ndef train(epoch):\n  network.train()\n  for batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = network(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % log_interval == 0:\n      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n        epoch, batch_idx * len(data), len(train_loader.dataset),\n        100. * batch_idx / len(train_loader), loss.item()))\n      train_losses.append(loss.item())\n      train_counter.append(\n        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n      torch.save(network.state_dict(), '/results/model.pth')\n      torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n```\n\n```python\ndef test():\n  network.eval()\n  test_loss = 0\n  correct = 0\n  with torch.no_grad():\n    for data, target in test_loader:\n      output = network(data)\n      test_loss += F.nll_loss(output, target, size_average=False).item()\n      pred = output.data.max(1, keepdim=True)[1]\n      correct += pred.eq(target.data.view_as(pred)).sum()\n  test_loss /= len(test_loader.dataset)\n  test_losses.append(test_loss)\n  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset),\n    100. * correct / len(test_loader.dataset)))\n```\n\n```python\ntest()\nfor epoch in range(1, n_epochs + 1):\n  train(epoch)\n  test()\n```\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n      warnings.warn(warning.format(ret))\n\n\u200b\n    Test set: Avg. loss: 2.3236, Accuracy: 913/10000 (9%)\n\n    Train Epoch: 1 [0/60000 (0%)] Loss: 2.359348\n    Train Epoch: 1 [6400/60000 (11%)] Loss: 0.912735\n    Train Epoch: 1 [12800/60000 (21%)] Loss: 0.791347\n    Train Epoch: 1 [19200/60000 (32%)] Loss: 0.675237\n    Train Epoch: 1 [25600/60000 (43%)] Loss: 0.598521\n    Train Epoch: 1 [32000/60000 (53%)] Loss: 0.471895\n    Train Epoch: 1 [38400/60000 (64%)] Loss: 0.458211\n    Train Epoch: 1 [44800/60000 (75%)] Loss: 0.328482\n    Train Epoch: 1 [51200/60000 (85%)] Loss: 0.386726\n    Train Epoch: 1 [57600/60000 (96%)] Loss: 0.434701\n    \n    Test set: Avg. loss: 0.2936, Accuracy: 9181/10000 (92%)\n    \n    Train Epoch: 2 [0/60000 (0%)] Loss: 0.758641\n    Train Epoch: 2 [6400/60000 (11%)] Loss: 0.531642\n    Train Epoch: 2 [12800/60000 (21%)] Loss: 0.474639\n    Train Epoch: 2 [19200/60000 (32%)] Loss: 0.587206\n    Train Epoch: 2 [25600/60000 (43%)] Loss: 0.557518\n    Train Epoch: 2 [32000/60000 (53%)] Loss: 0.563830\n    Train Epoch: 2 [38400/60000 (64%)] Loss: 0.340438\n    Train Epoch: 2 [44800/60000 (75%)] Loss: 0.486597\n    Train Epoch: 2 [51200/60000 (85%)] Loss: 0.517953\n    Train Epoch: 2 [57600/60000 (96%)] Loss: 0.418494\n    \n    Test set: Avg. loss: 0.2365, Accuracy: 9314/10000 (93%)\n    \n    Train Epoch: 3 [0/60000 (0%)] Loss: 0.397563\n    Train Epoch: 3 [6400/60000 (11%)] Loss: 0.190083\n    Train Epoch: 3 [12800/60000 (21%)] Loss: 0.423875\n    Train Epoch: 3 [19200/60000 (32%)] Loss: 0.422053\n    Train Epoch: 3 [25600/60000 (43%)] Loss: 0.380608\n    Train Epoch: 3 [32000/60000 (53%)] Loss: 0.266605\n    Train Epoch: 3 [38400/60000 (64%)] Loss: 0.343277\n    Train Epoch: 3 [44800/60000 (75%)] Loss: 0.305574\n    Train Epoch: 3 [51200/60000 (85%)] Loss: 0.679861\n    Train Epoch: 3 [57600/60000 (96%)] Loss: 0.372059\n    \n    Test set: Avg. loss: 0.2112, Accuracy: 9380/10000 (94%)\n\n\u200b\n\n## Validate the performance\n\n```python\nfig = plt.figure()\nplt.plot(train_counter, train_losses, color='blue')\nplt.scatter(test_counter, test_losses, color='red')\nplt.legend(['Train Loss', 'Test Loss'], loc='upper right')\nplt.xlabel('number of training examples seen')\nplt.ylabel('negative log likelihood loss')\nfig\n```\n\n![](https://bbs-img.huaweicloud.com/blogs/img/result.png?raw=true)\n\n## Sampling & Testing\n\n```python\nwith torch.no_grad():\n  output = network(example_data)\n```\n\n```python\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Prediction: {}\".format(\n    output.data.max(1, keepdim=True)[1][i].item()))\n  plt.xticks([])\n  plt.yticks([])\nfig\n```\n\n![](https://bbs-img.huaweicloud.com/blogs/img/digit.png?raw=true)\n"}