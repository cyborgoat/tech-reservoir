{"title": "Pre-trained Model --- BERT", "summary": "An introduction to BERT algorithm and a simple example to use this model", "author": "Junxiao Guo", "date": "2021-03-05", "tags": ["pre-trained model", "nlp"], "content": "---\ntitle: Pre-trained Model --- BERT\nsummary: An introduction to BERT algorithm and a simple example to use this model\nauthor: Junxiao Guo\ndate: 2021-03-05\ntags:\n  - \"pre-trained model\"\n  - nlp\n---\n\n## Abstract\n\nBERT \u5168\u79f0\u4e3a Bidirectional Encoder Representations from Transformers. BERT \u65e8\u5728\u901a\u8fc7\u5bf9\u6240\u6709\u5c42\u7684\u5de6\u53f3\u4e0a\u4e0b\u6587\u8fdb\u884c\u8054\u5408\u8c03\u8282,\u4ece\u672a\u6807\u8bb0\u7684\u6587\u672c\u4e2d\u9884\u8bad\u7ec3\u6df1\u5ea6\u53cc\u5411\u8868\u793a. \u56e0\u6b64,\u9884\u8bad\u7ec3\u7684 BERT \u6a21\u578b\u53ef\u4ee5\u4ec5\u901a\u8fc7\u4e00\u4e2a\u989d\u5916\u7684\u8f93\u51fa\u5c42\u8fdb\u884c\u5fae\u8c03,\u4ece\u800c\u4e3a\u5404\u79cd\u4efb\u52a1(\u4f8b\u5982\u95ee\u7b54\u548c\u8bed\u8a00\u63a8\u7406)\u521b\u5efa\u6700\u5148\u8fdb\u7684\u6a21\u578b,\u800c\u65e0\u9700\u5bf9\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u67b6\u6784\u8fdb\u884c\u5927\u91cf\u4fee\u6539.\n\n## Masked Language Model\n\nMLM (Masked Language Model) \u7075\u611f\u6765\u6e90\u4e8e\u5b8c\u5f62\u586b\u7a7a (Taylor, 1953): \u4ece\u53e5\u5b50\u4e2d\u5220\u9664\u4e00\u4e2a\u6216\u51e0\u4e2a\u5355\u8bcd\u5e76\u8981\u6c42\u5b66\u751f\u586b\u5199\u7f3a\u5931\u5185\u5bb9\u7684\u6d3b\u52a8. \u8be5\u53e5\u5b50\u53ef\u4ee5\u79f0\u4e3a\"stem\", \u5220\u9664\u7684\u672f\u8bed\u672c\u8eab\u79f0\u4e3a\"key\". MLM\u5c06\u8f93\u5165\u7684tokens\u8fdb\u884c\u968f\u673a\u63a9\u76d6, \u7136\u540e\u6839\u636e\u4e0a\u4e0b\u6587\u4fe1\u606f\u8bad\u7ec3\u6a21\u578b\u6765\u9884\u6d4b\u88ab\u63a9\u76d6\u7684tokens. MLM\u4e0e\u901a\u5e38\u7684 left-to-right \u6a21\u578b\u4e0d\u540c, MLM \u76ee\u6807\u4f7f\u8868\u793a\u80fd\u591f\u878d\u5408\u5de6\u53f3\u4e0a\u4e0b\u6587, \u8fd9\u4f7f\u5176\u80fd\u591f\u9884\u8bad\u7ec3\u6df1\u5ea6\u53cc\u5411 Transformer. \u9664\u4e86Masked Language Model,BERT\u8fd8\u4f7f\u7528\u4e86 \"Nets sentence prediction\" \u4efb\u52a1\u6765\u8054\u5408\u9884\u8bad\u7ec3 text-pair representations.\n\n## BERT\n\nBERT\u7684\u6574\u4f53\u6d41\u7a0b\u5305\u542b\u4e24\u4e2a\u6b65\u9aa4: `pre-training` and `fine-tuning`.\n\n\u5728 pre-training \u4e2d, \u6a21\u578b\u4f1a\u5728\u65e0\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bad\u7ec3. \u5728 fine-tuning \u8fc7\u7a0b\u4e2d, BERT \u9996\u5148\u6839\u636e pre-training \u8fc7\u7a0b\u4e2d\u5b66\u4e60\u5230\u7684\u53c2\u6570\u8fdb\u884c\u521d\u59cb\u5316, \u6240\u6709\u7684\u53c2\u6570\u4f1a\u901a\u8fc7\u6709\u76d1\u7763\u7684\u65b9\u5f0f\u6765\u8bad\u7ec3\u4e0b\u6e38\u4efb\u52a1. \u6bcf\u4e2a\u4e0b\u6e38\u4efb\u52a1\u90fd\u6709\u5355\u72ec\u7684\u5fae\u8c03\u6a21\u578b, \u5373\u4f7f\u5b83\u4eec\u662f\u4f7f\u7528\u76f8\u540c\u7684\u9884\u8bad\u7ec3\u53c2\u6570\u521d\u59cb\u5316\u7684.\n\n<img src='https://bbs-img.huaweicloud.com/blogs/img/paperfig-1.PNG'>\nBERT\u7684 pre-training \u548c fine-truning\u7684\u6574\u4f53\u6d41\u7a0b\n\nBERT\u7684\u7279\u70b9\u4e4b\u4e00\u5c31\u662f\u5b83\u5728\u4e0d\u540c\u4efb\u52a1\u4e0b\u7684\u7684\u7edf\u4e00\u5316\u7ed3\u6784, \u8fd9\u4f7f\u5f97\u80fd\u591f\u6700\u5c0f\u5316\u9884\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u7ed3\u6784.\n\n**Model Architecture**\n\nBERT\u7684\u7684\u6a21\u578b\u7ed3\u6784\u662f Multi-layer Bidirectional Transformer Encoder (\u591a\u5c42\u53cc\u5411Transformer\u7f16\u7801\u5668). \u5176\u4e2d `L` \u8868\u793a\u6a21\u578b\u7684\u5c42\u6570 (i.e., Transformer blocks), `A` \u8868\u793aself-attention heads.\n\nBERT\u4e3b\u8981\u63d0\u4f9b\u4e86\u4e24\u4e2a\u6a21\u578b\u7684\u7ed3\u679c:\n\n1. $BERT_{BASE}$ (L=12,H=768,A=12) \u603b\u8ba1\u53c2\u6570\u91cf\u4e3a110M.\n2. $BERT_{LARGE}$ (L=24,H=1024,A=16) \u603b\u8ba1\u53c2\u6570\u91cf\u4e3a340M.\n\n$BERT_{BASE}$\u7684\u7ed3\u6784\u4e0e*OpenAI GPT*\u7684\u7ed3\u6784\u5b8c\u5168\u76f8\u7b49, \u8be5\u8bbe\u8ba1\u4e3b\u8981\u662f\u4e3a\u4e86\u548c*OpenAI GPT*\u8fdb\u884c\u6bd4\u8f83, \u5176\u4e2d\u503c\u5f97\u6ce8\u610f\u7684\u662f: **BERT Transformer\u4f7f\u7528\u7684\u662fbidirectional self-attention, GTP Transformer \u4f7f\u7528\u7684\u662fconstrained self-attention, \u6240\u6709\u7684token\u53ea\u6ce8\u610f\u4e0a\u6587\u7684\u4fe1\u606f,\u5e76\u4e0d\u5305\u542b\u4e0b\u6587\u4fe1\u606f**\n\n**I/O Representations**\n\n\u4e3a\u4e86\u8ba9 BERT \u5904\u7406\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1,\u6a21\u578b\u7684\u8f93\u5165\u53ef\u4ee5\u660e\u786e\u5730\u8868\u793a\u5355\u4e2a\u53e5\u5b50\u548c\u53e5\u5b50\u5bf9(Sentence Pair)(e.g.,  <Question,Answer>), \u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u5355\u4e2a token sequence. \u5728\u6574\u4e2a\u5de5\u4f5c\u4e2d,\"\u53e5\u5b50\"\u53ef\u4ee5\u662f\u4efb\u610f\u8303\u56f4\u7684\u8fde\u7eed\u6587\u672c, \u800c\u4e0d\u662f\u5b9e\u9645\u7684\u8bed\u8a00\u53e5\u5b50. \"\u5e8f\u5217\" \u662f\u6307\u8f93\u5165\u5230 BERT \u7684 token sequence, \u53ef\u4ee5\u662f\u5355\u4e2a\u53e5\u5b50, \u4e5f\u53ef\u4ee5\u662f\u4e24\u4e2a\u6253\u5305\u5728\u4e00\u8d77\u7684\u53e5\u5b50.\n\nBERT \u4f7f\u7528\u4e86 *WordPiece embeddings* \u505a\u4e3a\u53c2\u8003, \u5176\u4e2d\u5305\u542b\u4e8630,000\u4e2a token. \u5bf9\u4e8e\u6240\u6709\u7684\u5e8f\u5217\u6765\u8bf4, \u9996\u4e2a token \u6c38\u8fdc\u90fd\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u5206\u7c7b token ([CLS]). \u4e0e token \u5bf9\u5e94\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7528\u4f5c\u5206\u7c7b\u4efb\u52a1\u7684\u805a\u5408\u5e8f\u5217\u8868\u793a(aggregate sequence representation). \u53e5\u5b50\u5bf9(Sentence Pair) \u540c\u6837\u7684, \u4f1a\u88ab\u6574\u5408\u8fdb\u5355\u72ec\u7684\u4e00\u4e2a\u5e8f\u5217. \u4e3a\u4e86\u80fd\u591f\u5c06\u6587\u672c\u4e2d\u7684\u53e5\u5b50\u8fdb\u884c\u533a\u5206, BERT\u4f7f\u7528\u4e86\u4e24\u79cd\u65b9\u5f0f, \u9996\u5148\u901a\u8fc7\u4e00\u4e2a\u7279\u6b8a token ([SEP]) \u6765\u8fdb\u884c\u533a\u5206, \u7136\u540e\u52a0\u5165\u4e00\u4e2a\u7ecf\u8fc7\u5b66\u4e60\u7684 embedding \u6765\u5224\u65ad\u6bcf\u4e00\u4e2a token \u5c5e\u4e8e sentence A \u8fd8\u662f sentence B.\n\n\u5df2\u77e5\u4e00\u4e2atoken\u7684\u60c5\u51b5\u4e0b, \u5b83\u5bf9\u5e94\u7684\u8f93\u5165\u8868\u5f81\u4e3a token \u672c\u8eab, \u5b83\u5bf9\u5e94\u7684 segment\u4ee5\u53ca position embedding \u7684\u7d2f\u52a0. \u5982\u4e0b\u56fe\u6240\u793a.\n\n![](https://bbs-img.huaweicloud.com/blogs/img/paperfig-2.PNG)\nBERT Input Respresentation\n\n### \u9884\u8bad\u7ec3BERT\n\nBERT\u4f7f\u7528\u4e86 BooksCorpus(800M \u5b57)\u548c\u82f1\u6587\u7ef4\u57fa\u767e\u79d1(2,500M \u5b57)\u505a\u4e3a\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\n\n#### Task #1: MLM (Masked Language Model)\n\n\u76f4\u89c2\u6765\u8bb2,\u6df1\u5ea6\u53cc\u5411\u6a21\u578b\u6bd4\u4ece\u5de6\u5230\u53f3\u6a21\u578b\u6216\u4ece\u5de6\u5230\u53f3\u548c+\u53f3\u5230\u5de6\u6a21\u578b\u7684\u6d45\u5c42\u7ea7\u8054\u66f4\u5f3a\u5927. \u4f46\u662f,\u6807\u51c6\u7684 conditional language models\u53ea\u80fd\u4ece\u5de6\u5230\u53f3\u6216\u4ece\u53f3\u5230\u5de6\u8fdb\u884c\u8bad\u7ec3, \u56e0\u4e3a\u53cc\u5411\u6761\u4ef6\u4f1a\u5141\u8bb8\u6bcf\u4e2a\u8bcd\u95f4\u63a5\"\u770b\u5230\u81ea\u5df1\", \u4e5f\u5c31\u662f\u8bf4, \u8fd9\u6837\u7684\u6761\u4ef6\u4f1a\u5141\u8bb8\u6a21\u578b\"\u4f5c\u5f0a\", \u4f7f\u5176\u5728\u4e00\u4e2a\u591a\u5c42\u8bed\u5883\u4e2d\u53ef\u4ee5\u8f7b\u677e\u9884\u6d4b\u76ee\u6807\u8bcd.\n\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u6837\u7684\u95ee\u9898, BERT\u968f\u673a\u5bf9\u4e00\u90e8\u5206\u8f93\u5165\u7684 token \u91c7\u53d6\u4e86 `mask` \u4f7f\u5176\u4e0d\u53ef\u89c1. BERT\u5bf9\u6bcf\u4e00\u4e2a\u5e8f\u5217\u91c7\u53d6\u4e8615%\u7684mask, \u5e76**\u53ea\u7528\u4e8e\u9884\u6d4b\u88abmask\u6389\u7684\u5355\u8bcd, \u800c\u4e0d\u662f\u9884\u6d4b\u6574\u4e2a\u8f93\u5165**\n\n\u4f46\u662f\u8fd9\u6837\u505a\u4f1a\u5bfc\u81f4\u53e6\u5916\u4e00\u4e2a\u95ee\u9898: \u8fd9\u6837\u7684\u8bad\u7ec3\u65b9\u5f0f\u4f1a\u5bfc\u81f4\u5728 fine tuning \u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6a21\u578b\u95f4\u7684\u5dee\u5f02\u6027, \u56e0\u4e3a\u5728 fine tuning \u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u51fa\u73b0  [MASK] token. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898, BERT\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e76\u4e0d\u603b\u662f\u5bf915%\u7684 token \u91c7\u53d6 mask, \u800c\u662f\u4ee580%\u6982\u7387\u8fdb\u884cmask , 10%\u4f1a\u968f\u673a\u9009\u53d6\u4e00\u4e2atoken\u4ee3\u66ff, \u6700\u540e\u5269\u4e0b\u768410% \u4f1a\u4fdd\u7559\u539f\u6709token. \u63a5\u7740, $T_i$ \u5219\u4f1a\u7528\u4e8e\u9884\u6d4b\u5b9e\u9645token (\u7ed3\u5408 corss entropy loss)\n\n#### Task #2: NSP (Next Sentence Prediction)\n\n\u4f20\u7edf\u7684\u673a\u5668\u95ee\u7b54 (Question Answering) \u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406 (Natural Language Inference) \u4efb\u52a1\u4e00\u822c\u662f\u57fa\u4e8e\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb, \u4e5f\u5c31\u662f\u8bf4, \u8fd9\u6837\u7684\u65b9\u5f0f\u65e0\u6cd5\u6355\u83b7\u5230\u57fa\u4e8e Language Model\u8bad\u7ec3\u5f97\u5230\u7684\u4fe1\u606f. \u4e3a\u4e86\u8bad\u7ec3\u4e00\u4e2a\u7406\u89e3\u53e5\u5b50\u5173\u7cfb\u7684\u6a21\u578b, BERT \u9488\u5bf9Binarized next sentence prediction\u4efb\u52a1\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3. \u5f53\u4e3a\u6bcf\u4e2a\u9884\u8bad\u7ec3\u793a\u4f8b\u9009\u62e9\u53e5\u5b50 A \u548c B \u65f6, 50% \u7684\u65f6\u95f4 B \u662f A \u4e4b\u540e\u7684\u5b9e\u9645\u4e0b\u4e00\u4e2a\u53e5\u5b50(\u6807\u8bb0\u4e3a IsNext), 50% \u7684\u65f6\u95f4\u5b83\u662f\u6765\u81ea\u8bed\u6599\u5e93\u7684\u968f\u673a\u53e5\u5b50(\u6807\u8bb0\u4e3a\u4f5c\u4e3a NotNext).\n\n### Fine-tuning BERT\n\n\u5bf9\u4e8e\u6d89\u53ca\u6587\u672c\u5bf9\u7684(text pair)\u7684\u5e94\u7528\u573a\u666f,\u4e00\u4e2a\u5e38\u89c1\u7684\u6a21\u5f0f\u662f\u5728\u5e94\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b (bidirectional cross attention) \u4e4b\u524d\u5bf9\u6587\u672c\u5bf9\u8fdb\u884c\u72ec\u7acb\u7f16\u7801.\u76f8\u53cd,BERT \u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u7edf\u4e00\u8fd9\u4e24\u4e2a\u9636\u6bb5,\u56e0\u4e3a\u7f16\u7801\u5177\u6709\u81ea\u6ce8\u610f\u529b\u7684\u8fde\u63a5\u6587\u672c\u5bf9\u53ef\u4ee5\u6709\u6548\u5730\u5305\u542b\u4e86\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b.\u5bf9\u4e8e\u6bcf\u4e2a\u4efb\u52a1,\u53ea\u9700\u5c06\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u63d2\u5165 BERT \u5e76\u5fae\u8c03\u6240\u6709\u53c2\u6570.\n\n\u5728\u8f93\u51fa\u7aef,token representation \u653e\u5165\u8f93\u51fa\u5c42\u7528\u4e8e\u6807\u8bb0\u7ea7\u4efb\u52a1,\u4f8b\u5982\u5e8f\u5217\u6807\u8bb0\u6216\u95ee\u7b54,\u800c [CLS] \u8868\u793a\u88ab\u8f93\u5165\u5230\u8f93\u51fa\u5c42\u8fdb\u884c\u5206\u7c7b,\u4f8b\u5982\u8574\u542b\u6216\u60c5\u611f\u5206\u6790.\n"}