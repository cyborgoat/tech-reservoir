{"title": "ALBERT", "summary": "An introduction to ALBERT algorithm and a simple example to use this model", "author": "Junxiao Guo", "date": "2022-08-05", "tags": ["pre-trained-model", "nlp"], "content": "---\ntitle: ALBERT\nsummary: An introduction to ALBERT algorithm and a simple example to use this model\nauthor: Junxiao Guo\ndate: 2022-08-05\ntags:\n  - \"pre-trained-model\"\n  - \"nlp\"\n---\n\n[\\[\u8bba\u6587\u94fe\u63a5\\]](https://arxiv.org/pdf/1909.11942.pdf)\n\n> \u672c\u6587\u672b\u4f1a\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8eTransformers(\u6df1\u5ea6\u5b66\u4e60\u5f00\u6e90\u5e93)\u7684\u7b80\u6613ALBERT\u7b97\u6cd5\u591a\u9009\u9898\u4efb\u52a1\u63a8\u7406Demo(\u6682\u4e0d\u63d0\u4f9bFine Tuning\u4ee3\u7801)\n\nALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS\n\n\u5728NLP\u6a21\u578b\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u63d0\u5347\u6a21\u578b\u89c4\u6a21\u5f80\u5f80\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8868\u73b0. \u4f46\u662f, \u5f88\u591a\u65f6\u5019\u7531\u4e8eGPU\u5185\u5b58\u6709\u9650\u7684\u539f\u56e0, \u5bfc\u81f4\u4e86\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u53d8\u957f. ALBERT\u63d0\u51fa\u4e86\u4e24\u79cd\u53c2\u6570\u538b\u7f29\u7684\u65b9\u6cd5\u6765\u964d\u4f4eBERT\u7684\u5185\u5b58\u5360\u7528,\u540c\u65f6\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6:\n\n- \u7b2c\u4e00\u79cd\u65b9\u6cd5\u662f\u901a\u8fc7\u5d4c\u5165\u53c2\u6570\u56e0\u5f0f\u5206\u89e3(Factorized Embedding Parameterization),\u5c06\u5de8\u5927\u7684\u8bcd\u5d4c\u5165\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u5c0f\u578b\u77e9\u9635.\n- \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662f\u8de8\u5c42\u53c2\u6570\u5171\u4eab(Cross-Layer Parameter Sharing).\n\n\u901a\u8fc7\u4ee5\u4e0a\u4e24\u79cd\u65b9\u6cd5, \u4e00\u4e2a\u8fd1\u4f3c\u4e8eBERT-large\u53c2\u6570\u914d\u7f6e\u7684ALBERT\u6a21\u578b\u53ea\u9700\u8981\u539f\u6a21\u578b$\\frac{1}{18}$\u7684\u53c2\u6570\u91cf\u5c31\u80fd\u8fbe\u52301.7\u500d\u7684\u8bad\u7ec3\u901f\u5ea6.\n\n## ALBERT\u6838\u5fc3\u7ed3\u6784\n\n### \u6a21\u578b\u7ed3\u6784\u9009\u62e9\n\nALBERT\u7684\u57fa\u7840\u6a21\u578b\u4e0eBERT\u975e\u5e38\u7c7b\u4f3c(Transformer Encoder + GELU nonlinearities). \u9996\u5148,\u6211\u4eec\u505a\u51fa\u4ee5\u4e0b\u5b9a\u4e49(\u4ee5\u4e0b\u5b9a\u4e49\u548cBERT\u539f\u6587\u4e2d\u7684\u53c2\u6570\u5b9a\u4e49\u8fdb\u884c\u4e86\u5bf9\u9f50):\n\n- E: Embedding size\n- L: Encoder layers\n- H: Hidden size\n\n\u5176\u4e2d, feed-forward/filter\u5c3a\u5bf8\u4e3a $4H$, attenion heads\u4e3a $H/64$\n\n### \u5d4c\u5165\u53c2\u6570\u5206\u89e3(Factorized embedding parameterization)\n\n\u5728BERT, XLNet, RoBERTa \u6a21\u578b\u4e2d, WordPiece embedding size E \u4e0e\u9690\u85cf\u5c42\u662f\u5b8c\u5168\u5bf9\u9f50\u7684, i.e.$E \\equiv H$, \u4f46\u662f\u8fd9\u6837\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u8bad\u7ec3\u548c\u5b9e\u7528\u6027\u6765\u8bf4\u5e76\u975e\u6700\u4f18, \u539f\u56e0\u5982\u4e0b\n\n- \u4ece\u6a21\u578b\u89d2\u5ea6\u6765\u8bb2, WordPiece embeeding\u7684\u610f\u4e49\u5728\u4e8e\u5b66\u4e60\u5230\u4e0a\u4e0b\u6587\u72ec\u7acb(context-independent)\u7684\u8868\u5f81, hidden-layer embeddings \u7684\u610f\u4e49\u5728\u4e8e\u5b66\u4e60\u5230\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587(context-dependent)\u7684\u8868\u5f81, \u7c7b\u4f3c\u4e8eBERT\u7ed3\u6784\u7684\u6a21\u578b\u7684\u5f3a\u52b2\u8868\u73b0\u4e3b\u8981\u6765\u6e90\u4e8e\u4e0a\u4e0b\u6587\u8868\u5f81\u7684\u83b7\u53d6. \u56e0\u6b64, \u5c06WordPiece embedding size \u548chidden-layer size \u89e3\u8026\u53ef\u4ee5\u66f4\u4f7f\u6a21\u578b\u66f4\u6709\u6548\u7684\u5229\u7528\u597d\u53c2\u6570, \u4e5f\u5c31\u662f\u8bf4, $H \\gg E$\n\n- \u4ece\u5b9e\u7528\u6027\u7684\u89d2\u5ea6\u6765\u8bb2, NLP\u4efb\u52a1\u4e00\u822c\u4f1a\u8981\u6c42vocabulary size $V$ \u975e\u5e38\u5927, \u5982\u679c $E\\equiv H$\u7684\u8bdd, \u63d0\u9ad8$H$\u4e5f\u4f1a\u63d0\u9ad8\u5d4c\u5165\u77e9\u9635\u7684\u5927\u5c0f,\u4e5f\u5c31\u662f$V\u00d7E$. \u8fd9\u6837\u4f1a\u5bfc\u81f4\u51fa\u73b0\u6570\u5341\u4ebf\u7ea7\u522b\u7684\u53c2\u6570, \u4f46\u662f\u5927\u90e8\u5206\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u90fd\u662f\u7a00\u758f\u7684.\n\n\u56e0\u6b64, ALBERT\u5c06\u5d4c\u5165\u53c2\u6570(embedding parameters)\u8fdb\u884c\u4e86\u77e9\u9635\u5206\u89e3, \u5206\u89e3\u4e3a\u4e86\u4e24\u4e2a\u76f8\u5bf9\u5c0f\u5f88\u591a\u7684\u77e9\u9635. \u4e0d\u50cf\u76f4\u63a5\u5c06one-hot \u5411\u91cf\u76f4\u63a5\u653e\u5165\u5de8\u5927\u7684\u7684\u9690\u85cf\u5c42($size=H$), \u800c\u662f\u5c06\u8fd9\u4e9b\u5411\u91cf\u5148\u6620\u5c04\u5230\u4e00\u4e2a\u66f4\u4f4e\u7ef4\u5ea6\u7684\u77e9\u9635\u7a7a\u95f4($size=E$)\u4e2d, \u518d\u6620\u5c04\u5230\u9690\u85cf\u5c42\u4e2d. \u901a\u8fc7\u77e9\u9635\u5206\u89e3, ALBERT\u5c06\u5d4c\u5165\u53c2\u6570\u7684\u77e9\u9635\u5927\u5c0f\u7531$O(V \\times H)$\u53d8\u6210\u4e86$O(V \\times E + E \\times H)$. \u8fd9\u79cd\u8f6c\u6362\u53ef\u4ee5\u5728\u5f53$H \\gg E$\u7684\u65f6\u5019\u5927\u5e45\u5ea6\u964d\u4f4e\u53c2\u6570\u91cf.  \n\n### \u8de8\u5c42\u53c2\u6570\u5171\u4eab(Cross-layer parameter sharing)\n\n\u53c2\u6570\u5171\u4eab\u7684\u65b9\u5f0f\u6709\u5f88\u591a,\u6bd4\u5982:\n\n- \u53ea\u5171\u4eab\u6b63\u5411\u4f20\u64ad(feed-forward network FFN)\u7684\u53c2\u6570\n- \u53ea\u5171\u4eab\u6ce8\u610f\u529b(attention)\u53c2\u6570\n- ...\n\nALBERT\u7684\u9ed8\u8ba4\u65b9\u5f0f\u4e3a\u5171\u4eab\u6240\u6709\u5c42\u7684\u6240\u6709\u53c2\u6570.\n\n### \u53e5\u95f4\u8fde\u8d2f\u6027\u635f\u5931(Inter-sentence coherence loss)\n\n\u9664\u4e86masked language modeling (MLM) loss\u4ee5\u5916, BERT\u8fd8\u4f7f\u7528\u4e86 next-sentence prediction (NSP) loss. NSP loss\u662f\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u4e24\u4e2a\u7247\u6bb5\u662f\u5426\u662f\u5728\u539f\u53e5\u4e2d\u8fde\u8d2f\u7684\u635f\u5931\u51fd\u6570: \u6b63\u6837\u672c\u901a\u8fc7\u62bd\u53d6\u539f\u6587\u672c\u4e2d\u7684\u8fde\u7eed\u7247\u6bb5\u6765\u83b7\u5f97; \u8d1f\u6837\u672c\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u6587\u672c\u4e2d\u7684\u53d8\u77ed\u6765\u751f\u6210; \u5176\u4e2d\u6b63\u8d1f\u6837\u672c\u7684\u6bd4\u4f8b\u76f8\u7b49. NSP\u7684\u76ee\u6807\u662f\u4e3a\u4e86\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\u7684\u80fd\u529b, \u4f8b\u5982\u4e00\u4e9b\u9700\u8981\u8fdb\u884c\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u4efb\u52a1. \u4f46\u662f\u5df2\u6709\u90e8\u5206\u7814\u7a76\u53d1\u73b0NSP\u7684\u8bbe\u8ba1\u4e0d\u7a33\u5b9a\u53ef\u9760,\u6240\u4ee5\u540e\u6765\u53c8\u79fb\u9664\u4e86NSP. ALBERT\u4e00\u6587\u63a8\u6d4bNSP\u4e0d\u5177\u6709\u6709\u6548\u6027\u7684\u539f\u56e0\u662f\u8be5\u65b9\u6cd5\u5728\u5355\u4e2a\u4efb\u52a1\u4e2d\u6df7\u6dc6\u4e86**\u4e3b\u9898\u9884\u6d4b(topic predicion)**\u548c**\u8fde\u8d2f\u6027\u9884\u6d4b(coherence prediction)**.\n\n\u57fa\u4e8e\u4ee5\u4e0a\u7684\u5206\u6790,ALBERT\u4ece\u7740\u91cd\u4e8e\u8fde\u8d2f\u6027\u4fe1\u606f\u7684\u89d2\u5ea6\u63d0\u51fa\u4e86**sentence-order predicion(SOP) loss**, SOP\u7684\u6b63\u6837\u672c\u4f7f\u7528\u4e86\u548cBERT\u5b8c\u5168\u76f8\u540c\u7684\u91c7\u6837\u65b9\u6cd5, \u4f46\u662f\u8d1f\u6837\u672c\u53d8\u6210\u4e86**\u6b63\u6837\u672c\u7684\u9006\u5e8f\u6837\u672c**, \u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f\u53ef\u4ee5\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u53e5\u95f4\u7684\u8fde\u8d2f\u6027\u7279\u5f81.\n\n### \u6a21\u578b\u914d\u7f6e\n\nTable 1. \u5c55\u793a\u4e86BERT\u548cALBERT\u7684\u53c2\u6570\u95f4\u533a\u522b. \u53ef\u4ee5\u770b\u5230, ALBERT-xlarge\u7684\u53c2\u6570\u91cf\u4ec5\u7ea6\u7b49\u4e8eBERT-base\u768460%\u5de6\u53f3\n\n<p>\n    <img src='https://bbs-img.huaweicloud.com/blogs/img/20211130/1638243561546013186.PNG'>\n    <center>Table 1: BERT \u548c ALBERT\u7684\u53c2\u6570\u914d\u7f6e</center>\n</p>\n\n## ALBERT\u591a\u9009\u9898\u4efb\u52a1\u63a8\u7406demo\n\n```python\nimport torch\nfrom transformers import AlbertTokenizer, AlbertForMultipleChoice\nfrom transformers import BertTokenizer, AlbertModel\n```\n\n```python\ntokenizer = AlbertTokenizer.from_pretrained('albert_chinese_small')\nmodel = AlbertForMultipleChoice.from_pretrained('albert_chinese_small')\n```\n\n    The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n    The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n    The class this function is called from is 'BertTokenizer'.\n    Some weights of the model checkpoint at pre_trained_models/albert_chinese_small were not used when initializing AlbertForMultipleChoice: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']\n    - This IS expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n    - This IS NOT expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n    Some weights of AlbertForMultipleChoice were not initialized from the model checkpoint at pre_trained_models/albert_chinese_small and are newly initialized: ['classifier.bias', 'classifier.weight']\n    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n```python\n# \u8bdd\u9898\nprompt = \"\u7b97\u6cd5\u7ec4\u7684\u6210\u5458\u4eec\u7cbe\u901a\u5404\u79cdAI\u7b97\u6cd5.\" \n\n# \u9009\u62e9\nchoice0 = \"\u6210\u90fd\u52a8\u7269\u56ed\u91cc\u7684\u5927\u718a\u732b\u4eba\u4eba\u90fd\u60f3\u6478\u4e00\u4e0b.\"\nchoice1 = \"\u4ed6\u4eec\u4e3b\u8981\u8d1f\u8d23AI\u7b97\u6cd5\u7684\u7814\u7a76\u548c\u843d\u5730.\"\nchoice2 = \"\u6211\u6ca1\u5403\u4e0a\u5409\u58eb\u679c.\"\nchoice3 = \"\u4f60\u5403\u996d\u4e86\u5417?\"\nchoices = [choice0,choice1,choice2,choice3]\nlabels_list = [0,1,0,0]\nlabels = torch.FloatTensor(labels_list).unsqueeze(0)  # choice0 is correct, , batch size 1\nsize = len(choices)\n```\n\n```python\nencoding = tokenizer([prompt for _ in range(size)],choices, return_tensors='pt', padding=True)\noutputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\nloss = outputs.loss\nlogits = outputs.logits\nprint(logits)\n```\n\n    tensor([[ 0.0100,  0.2225, -0.0217, -0.0517]], grad_fn=<ViewBackward0>)\n\n```python\nfrom torch import nn\nfrom torch import autograd\nm = nn.Softmax(dim=1)\nsoft_maxed_logits = m(logits)\nprint(soft_maxed_logits)\n```\n\n    tensor([[0.2412, 0.2983, 0.2337, 0.2268]], grad_fn=<SoftmaxBackward0>)\n\n```python\nresult = torch.argmax(soft_maxed_logits).detach().numpy()\nresult\n```\n\n    array(1, dtype=int64)\n\n```python\n# \u53cd\u9988\u6700\u7ec8\u9009\u62e9\nchoices[result]\n```\n\n    '\u4ed6\u4eec\u4e3b\u8981\u8d1f\u8d23AI\u7b97\u6cd5\u7684\u7814\u7a76\u548c\u843d\u5730.'\n"}