{"title": "Metrics to evaluate Question Answering models", "summary": "This section introduces different kinds of metrics to evaluate question answering models.", "author": "Junxiao Guo", "date": "2023-03-26", "tags": ["nlp", "question-answering"], "content": "---\ntitle: Metrics to evaluate Question Answering models\nsummary: This section introduces different kinds of metrics to evaluate question answering models.\nauthor: Junxiao Guo\ndate: 2023-03-26\ntags:\n  - nlp\n  - question-answering\n---\n\n\n## Metric1: Exact Match Metric\n\nThe exact match (EM) metric does what you would expect it to. It returns a boolean value, yes or no, as to whether our predicted text matches to our true text. Let's take the following answers as our examples:\n\n```python\nanswers = [{'predicted': 'France', 'true': 'France.'},\n           {'predicted': 'in the 10th and 11th centuries',\n            'true': '10th and 11th centuries'},\n           {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n           {'predicted': 'Denmark, Iceland and Norway',\n            'true': 'Denmark, Iceland and Norway'},\n           {'predicted': 'Rollo', 'true': 'Rollo,'}]\n```\n\nTo calculate the EM accuracy of our model using these five predictions, all we need to do is iterate through each prediction, and append a `1` where there is an exact match, or a `0` where there is not.\n\n```python\nem = []\n\nfor answer in answers:\n    if answer['predicted'] == answer['true']:\n        em.append(1)\n    else:\n        em.append(0)\n\n# then total up all values in em and divide by number of values\nsum(em)/len(em)\n```\n\n    0.4\n\nA 40% EM score, which doesn't look very good despite the fact that we got incredibly close on every single answer. This is one of the limitations of using the EM metric, but we can make it slightly more lenient. For example our first answer returns 'France' and 'France.', the only difference being the final punctuation which is included in the true answer (which is actually less correct that what our model predicted).\n\nWe can clean each side of our text before comparison to remove these minor differences and return an exact match. For this, we can use regular expressions. We will remove any character which is not a space, letter, or number.\n\n```python\nimport re\n\nem = []\n\nfor answer in answers:\n    pred = re.sub('[^0-9a-z ]', '', answer['predicted'].lower())\n    true = re.sub('[^0-9a-z ]', '', answer['true'].lower())\n    if pred == true:\n        em.append(1)\n    else:\n        em.append(0)\n\n# then total up all values in em and divide by number of values\nsum(em)/len(em)\n```\n\n    0.8\n\nNow we get a slightly better score of 80%, but this is still not representative of the models performance. Ideally, we want to be turning to more advanced metrics that can deal with more fuzzy logic. We will be covering one of those methods next.\n\n## Metric2: ROUGE\n\nROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. The name is deceptively complicated, because this is not a difficult metric to understand, and it's incredibly easy to implement.\n\n### What is ROUGE\n\nROUGE is actually a set of metrics, rather than just one. We will cover the main ones that are most likely to be used, starting with ROUGE-N.\n\n#### ROUGE-N\n\nROUGE-N measures the number of matching 'n-grams' between our model predicted answer and a 'reference'.\n\nAn n-gram is simply a grouping of tokens/words. A unigram (1-gram) would consist of a single word. A bigram (2-gram) consists of two consecutive words:\n\nOriginal: \"the quick brown fox jumps over\"\n\nUnigrams: ['the', 'quick', 'brown', 'fox', 'jumps', 'over']\n\nBigrams: ['the quick', 'quick brown', 'brown fox', 'fox jumps', 'jumps over']\n\nTrigrams: ['the quick brown', 'quick brown fox', 'brown fox jumps', 'fox jumps over']\n\nThe reference in our case is our true answer.\n\nWith ROUGE-N, the N represents the n-gram that we are using. For ROUGE-1 we would be measuring the match-rate of unigrams between our model output and reference.\n\nROUGE-2 and ROUGE-3 would use bigrams and trigrams respectively.\n\nOnce we have decided which N to use \u2014 we now decide on whether we\u2019d like to calculate the ROUGE recall, precision, or F1 score.\n\n#### Recall\n\nThe recall counts the number of overlapping n-grams found in both the model output and reference \u2014 then divides this number by the total number of n-grams in the reference. It looks like this:\n\n![recall](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/recall.jpg?raw=true)\n\nThis is great for ensuring our model is **capturing all of the information contained in the reference** \u2014 but this isn\u2019t so great at ensuring our model isn\u2019t just pushing out a huge number of words to game the recall score:\n\n![rouge-gaming-recall](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_gaming_recall.png?raw=true)\n\n#### Precision\n\nTo avoid this we use the precision metric \u2014 which is calculated in almost the exact same way, but rather than dividing by the reference n-gram count, we divide by the model n-gram count.\n\n![precision-1](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_precision_calc.png?raw=true)\n![precision-1](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_precision_fixes_recall.png?raw=true)\n\n#### F1-Score\n\nNow that we both the recall and precision values, we can use them to calculate our ROUGE F1 score like so:\n\n$$2*\\frac{precision*recall}{precision+recall}$$\n![f1-1](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_f1.png?raw=true)\n\nThat gives us a reliable measure of our model performance that relies not only on the model capturing as many words as possible (recall) but doing so without outputting irrelevant words (precision).\n\n#### ROUGE-L\n\nROUGE-L measures the longest common subsequence (LCS) between our model output and reference. All this means is that we count the longest sequence of tokens that is shared between both:\n\n![rouge-l](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_l.png?raw=true)\n\nThe idea here is that a longer shared sequence would indicate more similarity between the two sequences. We can apply our recall and precision calculations just like before \u2014 but this time we replace the match with LCS.\n\nFirst we calculate the LCS reacall & precision:\n\n![rouge-l](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_l_recall.png?raw=true)\n![rouge-l](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_l_precision.png?raw=true)\n\nAnd finally, we calculate the F1 score just like we did before:\n\n$$2*\\frac{0.29*0.66}{0.29+0.66} = 0.6$$\n\n#### ROUGE-S\n\nThe final ROUGE metric we will look at is the ROUGE-S \u2014 or skip-gram concurrence metric.\n\nNow, this metric is much less popular than ROUGE-N and ROUGE-L covered already \u2014 but it\u2019s worth being aware of what it does.\n\nUsing the skip-gram metric allows us to search for consecutive words from the reference text, that appear in the model output but are separated by one-or-more other words.\n\nSo, if we took the bigram \u201cthe fox\u201d, our original ROUGE-2 metric would only match this if this exact sequence was found in the model output. If the model instead outputs \u201cthe brown fox\u201d \u2014 no match would be found.\n\nROUGE-S allows us to add a degree of leniency to our n-gram matching. For our bigram example we could match by using a skip-bigram measure:\n\n![rouge-s](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_s_recall.png?raw=true)\n![rouge-s](https://github.com/cyborgoat/tech-reservoir/blob/main/assets/images/QA_metrics/rouge_s_precision.png?raw=true)\n\nThen we calculate the F1 score as before\n\n#### Cons\n\nROUGE is a great evaluation metric but comes with some drawbacks. In-particular, ROUGE does not cater for different words that have the same meaning \u2014 as it measures syntactical matches rather than semantics.\n\nSo, if we had two sequences that had the same meaning \u2014 but used different words to express that meaning \u2014 they could be assigned a low ROUGE score.\n\nThis can be offset slightly by using several references and taking the average score, but this will not solve the problem entirely.\n\nNonetheless, it\u2019s a good metric which is very popular for assessing the performance of several NLP tasks, including machine translation, automatic summarization, and for us, question-and-answering\n\n### Apply Rouge In Python\n\nWe've worked through the theory of the ROUGE metrics and how they work. Fortunately, implementing these metrics in Python is incredibly easy thanks to the Python rouge library.\n\nWe can install the library through pip:\n\n```shell\npip install rouge\n```\n\nAnd scoring our model output against a reference is as easy as this:\n\n```python\nfrom rouge import Rouge\n\nmodel_out = 'hello to the world'\nreference = 'hello world'\n\n# initialize the rouge object\nrouge = Rouge()\n\n# get the scores\nrouge.get_scores(model_out, reference)\n```\n\n    [{'rouge-1': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223},\n      'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n      'rouge-l': {'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}}]\n\nThe get_scores method returns three metrics, ROUGE-N using a unigram (ROUGE-1) and a bigram (ROUGE-2) \u2014 and ROUGE-L.\n\nFor each of these, we receive the F1 score $f$, precision $p$, and recall $r$.\n\nLet's apply this to our set of five answers and see what we get. First, we need to define the answers list.\n\n```python\nanswers = [{'predicted': 'France', 'true': 'France.'},\n           {'predicted': 'in the 10th and 11th centuries',\n            'true': '10th and 11th centuries'},\n           {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n           {'predicted': 'Denmark, Iceland and Norway',\n            'true': 'Denmark, Iceland and Norway'},\n           {'predicted': 'Rollo', 'true': 'Rollo,'}]\n```\n\nThen we need to reformat this list into two lists, one for our predictions model_out and another for the true answers reference:\n\n```python\nmodel_out = [ans['predicted'] for ans in answers]\n\nreference = [ans['true'] for ans in answers]\n```\n\n```python\nmodel_out\n```\n\n    ['France',\n     'in the 10th and 11th centuries',\n     '10th and 11th centuries',\n     'Denmark, Iceland and Norway',\n     'Rollo']\n\nNow we can pass both of these lists to the rouge.get_scores method to return a list of results:\n\n```python\nrouge.get_scores(model_out, reference)\n```\n\n    [{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n      'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n      'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n     {'rouge-1': {'r': 1.0, 'p': 0.6666666666666666, 'f': 0.7999999952000001},\n      'rouge-2': {'r': 1.0, 'p': 0.6, 'f': 0.7499999953125},\n      'rouge-l': {'r': 1.0, 'p': 0.6666666666666666, 'f': 0.7999999952000001}},\n     {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n      'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n      'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n     {'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n      'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995},\n      'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}},\n     {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n      'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n      'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n\nIdeally, we want to get average metrics for all answers, we can do this by adding avg=True to the get_scores method.\n\n```python\nrouge.get_scores(model_out, reference, avg=True)\n```\n\n    {'rouge-1': {'r': 0.8, 'p': 0.7333333333333333, 'f': 0.7599999960400001},\n     'rouge-2': {'r': 0.6, 'p': 0.52, 'f': 0.5499999970625},\n     'rouge-l': {'r': 0.8, 'p': 0.7333333333333333, 'f': 0.7599999960400001}}\n\nAnd that's it, we've explored a few more insightful metrics for measuring our Q&A model performance. Going forwards, we'll be using ROUGE a lot, so it's good to get familiar with.And that's it, we've explored a few more insightful metrics for measuring our Q&A model performance. Going forwards, we'll be using ROUGE a lot, so it's good to get familiar with.\n"}