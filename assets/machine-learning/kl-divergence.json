{"title": "KL Divergence", "summary": "KL divergence or relative entropy is a measure of how one probability distribution is different from a reference probability distribution.", "author": "Junxiao Guo", "date": "2023-01-29", "tags": ["machine-learning", "similarity"], "content": "---\ntitle: KL Divergence \nsummary: KL divergence or relative entropy is a measure of how one probability distribution is different from a reference probability distribution.\nauthor: Junxiao Guo\ndate: 2023-01-29\ntags:\n  - machine-learning\n  - similarity\n---\n\nKL divergence or relative entropy is a measure of how one probability distribution is different from a reference probability distribution.\n\n## Core concept\n\n> Calculate the distance between two probability distributions\n\n## Log Likelihood Ratio\n\nLet's say we have data $X={x_1,x_2,...,x_n}$, with two different distributions $p_\\theta$ and $q_\\phi$, and we want to calculate the difference between these two distributions for $x_i$, they most straightforward way could be:\n\n$$p_\\theta(x_i) - p_\\phi(x_i)$$\n\nIf we take a log on both distributions (which prevents from rounding zero), the formula goes to:\n\n$$log{p_\\theta(x_i)} - \\log{p_\\phi(x_i)}$$\n\nto represent this formula in another way,\n\n$$\\log{\\left[\\frac{p_\\theta(x_i)}{{p_\\phi(x_i)}}\\right]}$$\n\nwhich is called the `Log Likelihood Ratio`.\n\n## Expected Value\n\nBefore we go to any further, let's think about what we are truly looking for: **A number which represents the average difference between two distributions.**\n\nSince we are dealing with random variables, there is no such term for \"average\", but `Expected Value` instead.\n\nThe expected value for `discrete random variable` is:\n\n$$\\mathbb{E}_{p_\\theta}\\left[h\\left(X\\right)\\right] = \\sum_{i=1}^{\\inf}{h\\left(x_i\\right)}{p_\\theta}\\left(x_i\\right)$$\n\n> This is also called as weight average of instances of random variables.\n\nfor `continuous random variable`, the formula become:\n\n$$\\mathbb{E}_{p_\\theta}\\left[h\\left(X\\right)\\right] = \\int_\\mathbb{R}{h\\left(x_i\\right)}{p_\\theta}\\left(x_i\\right)$$\n\n## Formula\n\nIf we look closer, the `Log Likelihood Ratio` is just a function of random variable, since we are looking for the \"average\" difference between two distributions, we apply the weight $p_\\theta$ and the formula becomes:\n\nFor distributions P and Q of a `continuous random variable`, the Kullback-Leibler divergence is computed as an integral:\n$$D_{KL}\\left(P \\parallel Q\\right) = \\int{p\\left(x\\right)\\log{\\left[\\frac{p\\left(x\\right)}{q\\left(x\\right)}\\right]}}dx$$\n\nif P and Q represent the probability distribution of a `discrete random variable`, the Kullback-Leibler divergence is calculated as a summation:\n\n$$D_{KL}\\left(P \\parallel Q\\right) = \\sum{p\\left(i\\right)\\log{\\left[\\frac{p_i\\left(x\\right)}{q_i\\left(x\\right)}\\right]}} $$\n\n## Example: Calculating KL Divergence in Python\n\nWe can make the KL divergence concrete with a worked example.\n\nConsider a random variable with three events as different colors. We may have two different probability distributions for this variable; for example:\n\n```python\n...\n# define distributions\nevents = ['red', 'green', 'blue']\np = [0.10, 0.40, 0.50]\nq = [0.80, 0.15, 0.05]\n```\n\nWe can plot a bar chart of these probabilities to compare them directly as probability histograms.\n\nThe complete example is listed below.\n\n```python\n# plot of distributions\nfrom matplotlib import pyplot\n# define distributions\nevents = ['red', 'green', 'blue']\np = [0.10, 0.40, 0.50]\nq = [0.80, 0.15, 0.05]\nprint('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n# plot first distribution\npyplot.subplot(2,1,1)\npyplot.bar(events, p)\n# plot second distribution\npyplot.subplot(2,1,2)\npyplot.bar(events, q)\n# show the plot\npyplot.show()\n```\n\n    P=1.000 Q=1.000\n\n![png](https://dsm01pap004files.storage.live.com/y4mKWO3ExagJgVU1n8joYNknuL29GQsQv53mX79lZ7PiT2622JtM_3rJ95w4xo1OpTqe6KDH9aT98H1uTPDvUCvHNKdFHKaZtganRebMJtNxWDfqEO7v9uduplA6KzTYVVJRLd7jKWp2d9VKnT4hyvJnrRmR-i2IFc2wK_R6VlGOQRPvwbLiUeUMLC9slDAjpXV?width=547&height=413&cropmode=none)\n\nRunning the example creates a histogram for each probability distribution, allowing the probabilities for each event to be directly compared.\n\nWe can see that indeed the distributions are different.\n\nNext, we can develop a function to calculate the KL divergence between the two distributions.\n\nWe will use log base-2 to ensure the result has units in bits.\n\nWe can then use this function to calculate the KL divergence of P from Q, as well as the reverse, Q from P.\n\n```python\n# example of calculating the kl divergence between two mass functions\nfrom math import log2\n\n# calculate the kl divergence\ndef kl_divergence(p, q):\n return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n\n# define distributions\np = [0.10, 0.40, 0.50]\nq = [0.80, 0.15, 0.05]\n# calculate (P || Q)\nkl_pq = kl_divergence(p, q)\nprint('KL(P || Q): %.3f bits' % kl_pq)\n# calculate (Q || P)\nkl_qp = kl_divergence(q, p)\nprint('KL(Q || P): %.3f bits' % kl_qp)\n```\n\n    KL(P || Q): 1.927 bits\n    KL(Q || P): 2.022 bits\n"}