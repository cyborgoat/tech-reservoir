---
title: KL Divergence
description: KL divergence or relative entropy is a measure of how one probability distribution is different from a reference probability distribution. 
author: junxiao guo
created_on: 2023-1-09
---