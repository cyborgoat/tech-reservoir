{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67640d1e-b760-4e88-a9cf-6b5ed2a0780f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b9d1c970648db831e30727b4dc455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86623ee1-10e0-4413-9b3b-a54fd360835d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: {'content': 'Â¶ÇÊûúÊÇ®ÁöÑÁÄèË¶ΩÂô®ÁÑ°ÂÆâË£ù Flash PlayerÔºåË´ãÊîπÁî® Google Chrome„ÄÅFirefox Êàñ Microsoft Edge Á≠âÁÄèË¶ΩÂô®ÔºåÈÄ≤ÂÖ•Êñ∞ÁâàÊú¨ÂåÖÂªÇ„ÄÇÂ¶ÇÊûúÊÇ®‰∏ÄÂÆöË¶Å‰ΩøÁî® Flash PlayerÔºåË´ãÁúã Flash PlayerÁÑ°Ê≥ï‰ΩøÁî®Ëß£Ê±∫Ëæ¶Ê≥ïÊïôÂ≠∏„ÄÇ‰∏ãËºâ Chrome ÁÄèË¶ΩÂô®‰∏ãËºâ Firefox ÁÄèË¶ΩÂô®‰∏ãËºâ Edge ÁÄèË¶ΩÂô®', 'warc_headers': {'warc-type': 'conversion', 'warc-refers-to': '<urn:uuid:07418760-251b-4ce7-90c2-5338261d7287>', 'content-type': 'text/plain', 'warc-date': '2021-11-28T11:10:49Z', 'warc-target-uri': 'http://1232785.aiavtv7f.com/V4/?AID=151524&FID=1232785&WEBID=AVSHOW', 'content-length': '298', 'warc-identified-content-language': 'zho,eng', 'warc-record-id': '<urn:uuid:915d1507-d1e1-49f1-b086-d9d94f7050b1>', 'warc-block-digest': 'sha1:KCOWZRQDU5K6TUFRZB3GSJR4DG3QOUPG'}, 'metadata': {'identification': {'label': 'zh', 'prob': 0.9202119}, 'annotation': ['tiny'], 'sentence_identifications': [{'label': 'zh', 'prob': 0.9202119}]}}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/zh_meta_part_1.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    print(f\"result: {result}\")\n",
    "    print(isinstance(result, dict))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56341bd-225d-4d58-bac8-90c46095c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./model/cyborgoat-small-vocab.json', './model/cyborgoat-small-merges.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install tokenizers\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "contents = [json.loads(x)['content'] for x in json_list]\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator():\n",
    "    batch_length = 1000\n",
    "    for i in range(0, len(contents), batch_length):\n",
    "        yield contents[i]\n",
    "\n",
    "\n",
    "# And finally train\n",
    "tokenizer.train_from_iterator(batch_iterator(), length=len(contents), vocab_size=52_000,min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\"./models\", \"cyborgoat-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04c210cf-1eb6-4c4b-add2-cb4d2ec422b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./models/cyborgoat-small-vocab.json\",\n",
    "    \"./models/cyborgoat-small-merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fa3afd9-c8b0-4430-821b-28170e6f6ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"‰Ω†Â•ΩÂêóÂÖÑÂºü„ÄÇ\")\n",
    "output2 = tokenizer.encode(\"‰Ω†Â•ΩÂêóÂÖÑÂºü„ÄÇ\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca3d99b7-20d1-428f-8bca-126e0f9d2525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '√§¬Ω≈Ç', '√•¬•¬Ω', '√•ƒ≤ƒπ', '√•ƒßƒ¶', '√•¬º≈Å', '√£ƒ¢ƒ§', '</s>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7776c1d3-b3f9-4713-8841-9f81f442ce3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 996, 439, 6667, 16808, 6127, 409, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30751c47-efe3-4027-b5b7-ed9263881467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 996, 439, 6667, 16808, 6127, 409, 2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5a21b6-ec91-4540-834d-606b2424620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorpustoDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \"./models/cyborgoat-small-vocab.json\",\n",
    "            \"./models/cyborgoat-small-merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = contents[:1000] if evaluate else contents\n",
    "\n",
    "        # src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")\n",
    "        # for src_file in src_files:\n",
    "        #     print(\"üî•\", src_file)\n",
    "        #     lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "        #     self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We‚Äôll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41756cbb-bb8b-4236-a3e1-05e92348dbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
